# Early Detection of Autism Spectrum Disorder Using Deep Learning
# Introduction

Autism spectrum disorder (ASD) is a lifelong developmental disability that affects communication and behavior. It is the fastest-growing developmental disability, and prevalence rates continue to rise incredibly. Current estimates indicate that about 1.5% of the world’s population is on the autism spectrum. Therefore, it is important for our classifier to work with high accuracy and precision scores since screening is an important factor in detecting ASD. The misclassification rates need to be low since not detecting an ASD can be dangerous, and treating it in time is also important. At the same time, it is important to note that classifying a case that does not have ASD as someone that has ASD can be worse since they do not need to undergo treatment; therefore, its important for these algorithms to be precise. In the current dataset, we have ten questions that are asked of the patients and other metrics such as gender, ethnicity, jaundice detection, etc. Based on these factors, we are trying to screen individuals for ASD and recommend further diagnoses for the same.

# Data Preprocessing and Cleaning

Since the dataset was highly imbalanced, we used random oversampling to balance the dataset for classification. We further filled null values for features like ethnicity and relation with their null values since there were 95 such cases, and completely excluding them would be a big loss, given the number of data points we have. We imputed these with their corresponding mode values as they are more likely to be there. We hot-encoded the categorical variables and created new columns for the neural network to comprehend and used the ‘Autism’ column as our target variable, which was created by further diagnoses of the patients. We also standardized the dataset so that no feature was given more importance than the other and are treated equally. We then divided the dataset into the train, validation, and test sets to feed it to the neural networks. We picked 80% of the dataset for training, 10% for validation, and the remaining 10% for testing. We did the same for all 4 cases. We then used TensorFlow to make tensors, which makes it easier for handling our data when performing transformations while dealing with neural networks. 

# Model Implementation

We created base models for the 4 cases that we are considering: The adult dataset, the Adolescents dataset, the Toddler dataset, and the combined dataset. The architecture for these base models was set as:

One input layer with ReLU activation, a hidden layer with ReLU activation, and an output layer with a sigmoid activation function. Along with this, we used the Adam optimizer and binary cross entropy along with accuracy as the metric for evaluation. 

For the Adult dataset, we have used The first dense layer has 512 neurons and uses the ReLU activation function, It also applies L2 regularization with a strength of 0.03 and 5 hidden layers with ReLU activation functions and dropout layers to reduce overfitting. The final dense layer has a single neuron with a sigmoid activation function.

For the Adolescents dataset, we have used a neural network model that comprises an input layer, four hidden layers, and an output layer. The input layer takes the input of the shape of training data which depends on the number of features in the input data. The hidden layers have ReLU activation functions and are followed by dropout layers with a rate of 0.2. The output layer is a single neuron with a sigmoid activation function and outputs a probability value between 0 and 1.

For the Toddlers dataset, the input layer has 32 neurons since it takes shape of the input set, which depends on the number of features in the input data. The first hidden layer has 32 neurons, uses the ReLU activation function, and applies L2 regularization with a strength of 0.01. The subsequent hidden layers consist of 128, 256, and 32 neurons, respectively, and also use the ReLU activation function. All the hidden layers also have a dropout layer after them with a rate of 0.2. The output layer consists of a single neuron with a sigmoid activation function that outputs a probability value between 0 and 1. 

For the combined dataset, the first layer is a Dense layer with 512 units, ReLU activation function, and an input dimension that is the same as the combined dataset. The kernel_regularizer argument specifies L2 regularization with a regularization strength of 0.03. There are 8 hidden layers with ReLU activation functions, which are commonly used for deep neural networks, along with dropout layers for regularization to prevent overfitting. The output layer is a Dense layer with 1 unit and a sigmoid activation function.
